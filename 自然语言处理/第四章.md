
**机器学习：**
让计算机从数据中自动学习，得到某种知识

**词袋模型：** ^5f7b03

在词袋模型中，文本被表示为一个向量，其中每个维度对应一个词语，而向量的值表示该词语在文本中的出现次数或者其他统计量，如词频（Term Frequency）或逆文档频率（Inverse Document Frequency）等。

词袋模型的基本步骤如下：

1. 构建词汇表：收集文本中的所有不重复词语，构建一个词汇表。
2. 特征向量表示：对于每个文本，根据词汇表中的词语，在特征向量中对应的位置上记录词语的出现频率或其他统计量。
3. 特征向量归一化：可以对特征向量进行归一化操作，如将词频除以文本中总词数，以减少文本长度的影响。

词袋模型的优点是简单直观，易于实现和理解。它可以用于文本分类、情感分析、信息检索等任务。然而，词袋模型忽略了词语的顺序和语义信息，无法捕捉到词语之间的关系和上下文信息。因此，在处理一些复杂的自然语言任务时，词袋模型的表现可能有限。为了解决这个问题，可以使用更高级的模型，如n-gram模型、词嵌入模型等，以更好地捕捉词语之间的关联和语义信息。
![[Pasted image 20230702165546.png]]

**机器学习三个基本要素：**

模型：模型是机器学习的核心组成部分，它是对数据进行学习和预测的数学描述。模型可以是线性回归模型、支持向量机、决策树、神经网络等。模型的选择取决于问题的性质和数据的特征。

学习准则： 学习准则是用于衡量模型预测结果与真实值之间的差异的函数。学习准则的选择取决于具体的问题和任务。例如，对于回归问题，常用的学习准则是均方误差（Mean Squared Error），对于分类问题，常用的学习准则是交叉熵损失（Cross-Entropy Loss）。

优化算法： 优化算法用于调整模型的参数，以最小化学习准则的值。优化算法的目标是找到使损失函数最小化的最优参数组合。常见的优化算法包括**梯度下降**、**随机梯度下降**、牛顿法、拟牛顿法等。这些算法通过迭代地更新模型的参数，使模型逐渐逼近最优解。

**监督学习**：训练集数据有标注（分类，回归，结构化学习）

**无监督学习**：训练集数据无标注（密度估计，聚类，特征抽取\\降维）
*还有弱监督学习和半监督学习*

**泛化与正则化**：为了解决过拟合问题，**引入参数的正则化**来**限制模型的能力**

**线性分类器：**

线性分类器是一种常见的分类算法，它基于线性模型对数据进行分类。线性分类器通过计算输入特征的线性组合，并将其映射到一个预定义的类别标签上。

线性分类器的一般形式可以表示为以下公式：

y = sign(w^T * x + b)

其中，

- y 是预测的类别标签（+1 或 -1）。
- x 是输入特征向量。
- w 是权重向量，用于表示特征的重要性。
- b 是偏置项，用于平移分类器的决策边界。
- w^T 是 w 的转置。

sign() 是符号函数，根据线性组合的结果判断样本所属的类别。如果 w^T * x + b 大于零，则预测为正类（+1），否则预测为负类（-1）。

线性分类器的目标是找到最优的权重向量 w 和偏置项 b，使得分类器能够最好地将不同类别的样本分开。这通常通过最小化损失函数、使用梯度下降等优化算法来实现。常见的线性分类器包括感知机、支持向量机（SVM）和逻辑回归等。

**逻辑回归（logistic regression）：**

逻辑回归是一种常用的分类算法，用于将样本分为两个不同的类别。逻辑回归通过将线性模型的输出通过一个逻辑函数（也称为sigmoid函数）进行映射，将连续的预测值转化为概率值。

逻辑回归的一般形式可以表示为以下公式：

p(y=1|x) = sigmoid(w^T * x + b)

其中，

- p(y=1|x) 是给定输入特征 x 条件下，样本属于类别 1 的概率。
- sigmoid() 是逻辑函数，用于将线性组合的结果转化为概率值。逻辑函数的公式为 sigmoid(z) = 1 / (1 + exp(-z))，其中 exp() 是指数函数。
- x 是输入特征向量。
- w 是权重向量，用于表示特征的重要性。
- b 是偏置项，用于平移分类器的决策边界。
- w^T 是 w 的转置。

逻辑回归的目标是找到最优的权重向量 w 和偏置项 b，使得模型能够最好地拟合训练数据，并对新样本进行准确的分类。通常使用最大似然估计或正则化方法来优化逻辑回归模型的参数。



softmax
Softmax是一种常用的数学函数，常用于机器学习中的分类问题。它将一组任意实数值转化为概率分布，用于表示多个类别的概率。
Softmax函数的定义如下：
给定一组实数值![[Pasted image 20230703094904.png]]，Softmax函数将每个值转化为一个介于0和1之间的概率值，满足以下公式：
![[Pasted image 20230703094855.png]]
其中，是自然对数的底数（约等于2.71828）。Softmax函数通过指数化每个输入值，并将它们归一化为总和为1的概率分布。
Softmax函数常用于多分类问题中，例如图像分类、情感分析等任务。它可以将模型的输出转化为各个类别的概率分布，从而可以选择概率最高的类别作为预测结果。
需要注意的是，Softmax函数具有平移不变性，即对输入值同时增加或减少一个常数，不会改变输出概率分布的相对大小。这种性质使得Softmax函数在训练神经网络时特别有用，因为可以避免数值上溢或下溢的问题。


前馈神经网络（FNN）也叫多层感知机（mlp）
FNN/MLP由输入层、隐藏层和输出层组成，每一层都由多个神经元组成。每个神经元接收来自上一层的输入，并通过激活函数对输入进行非线性变换，然后将结果传递到下一层。信息在网络中单向传播，不会形成回路，因此称为前馈网络。
FNN/MLP的训练过程通常使用反向传播算法（Backpropagation）来进行参数更新。反向传播算法通过计算损失函数对网络参数的梯度，然后根据梯度对参数进行调整，以最小化损失函数。
FNN/MLP在机器学习和深度学习中广泛应用，特别适用于解决分类和回归问题。通过增加隐藏层的数量和神经元的数量，FNN/MLP可以逼近任意复杂的非线性函数，具有较强的表达能力。
然而，FNN/MLP也存在一些限制，如容易陷入局部最优解、对初始权重敏感等。为了克服这些问题，研究者们提出了各种改进的神经网络结构和训练方法，如卷积神经网络（CNN）、循环神经网络（RNN）、深度残差网络（ResNet）等。这些模型在不同领域取得了重要的突破和应用。

卷积神经网络
卷积神经网络（CNN）是一种主要用于处理具有网格结构数据（如图像）的神经网络。它通过引入卷积层和池化层来有效地捕捉局部特征，并通过全连接层进行分类或回归。CNN的核心思想是权重共享和局部感知，即通过共享权重来减少参数数量，并通过局部感受野来捕捉输入数据的局部结构。这使得CNN在图像分类、目标检测、图像生成等计算机视觉任务中取得了巨大成功。

循环神经网络
循环神经网络（RNN）是一种主要用于处理序列数据（如语音、文本）的神经网络。RNN通过引入循环连接来处理序列中的时序信息，使得前面的输入能够影响后续的输出。这种循环结构使得RNN可以对序列数据进行建模，例如语言模型、机器翻译、语音识别等任务。然而，传统的RNN存在梯度消失或梯度爆炸的问题，为了解决这个问题，研究者提出了一些改进的RNN结构，如长短期记忆网络（LSTM）和门控循环单元（GRU），它们能够更好地捕捉长期依赖关系。

注意力机制

自注意模型（自注意模型一般需要加入位置编码信息来进行修正）





