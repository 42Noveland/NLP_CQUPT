**词的表示：**
        独热编码（独热表示假设所以词之间是独立无关的）

**分布式语义假设**： 词的含义可以有其上下文的分布进行表示
   基于此，人们可以利用大规模的未标注文本数据，根据每个词的上下文分布来学习词的表示，从而使得上下文分布接近的词，在词表示空间内也距离较近。

布朗聚类
        针对独热表示中不同词之间相互独立的问题，一种简单的解决办法是对词进行聚类：利用分布式语义假设，可以根据词的上下文分布特征将相似的词聚为一类。由此所得到的聚类表示在一定程度上建立了词与词之间的联系，但是仍然不具备对同一类别中词与词之间的细粒度区分能力。于是，Br0wm等人于1992年提出一种对于词的层次化聚类表示学习算法。，在该算法中，词表中的每个词是树末端的叶子节点，由根节点到每个词之间的路径（由0和1构成的比特串）则为相应的词表示。这种词表示也被称为布朗聚类(Brown cluster)表示。如图5.l所示，在词的布朗聚类表示中，可以通过使用不同长度的前缀来表示不同粒度下词与词之间的语义相关度。

   为了进一步提开词表示的表达能力，人们提出了**分布式表示**。在**分布式表示**中，词被表示为低维且稠密的实值向量。本节将介绍两种典型的分布式表示学习方法，分别是**潜在语义分析**(latent semantic analysis,LSA）以及**神经词嵌入**(neural word embedding).

潜在语义分析

上述方法在理论上完全可行，而在实操时却存在运行效率上的瓶颈。其原因主要在于输出层常常具有较高的维度，从而影响了softmax函数的计算效率。对于多类分类问题来说，当目标类别空间非常大时，往往需要对原始的分类方法进行一些近似或者变型。以下介绍两种近似方法，分别是潜在语义分析,以及一种更高效的基于负采样 negative sampling)的方法。

**评价**：
对于不同学习方法得到的词向量，通常可以根据其对词义相关性或者类比推理性的表达能力来进行评价，这种方式属于内部任务评价方法，在实际任务中则需要根据下游任务的性能指标来进行判断，也称为外部任务评价方法。
1. 词义相关性：对词义相关性的度量是词向量的重要性质之一，因此可以 根据词向量对于词义相关性的表达能力来衡量词向量的好坏。利用词向量低维稠密和连续的特性，可以方便的度量任意两个词之间的相关性。
![[Pasted image 20230702212911.png]]
2. 类比推理性：类比推理是对于词向量的另一种常用的内部评价方法。人们通过对词向量的分析发现，如果两个词对（wa,wb）与（wc,wd）之间满足某种词法或者语义关系上的类比，那么他们在词向量空间中存在v(wb) - v(wa) = v(wd)-(wc)的性质

![[Pasted image 20230702212856.png]]


**短语与句子表示**：    
   获得词的表示之后，通过对词表示进行组合计算，可以进一步得到短语或句子，甚至是篇章等更大语言单元的向量表示，这一过程也被称作语义组合(semantie composition)a所得到的表示可以经由进一步的运算来完成下游的任务，如文本分类、文本检索等。例如，在情感分类任务中，对于一段给定的文本，希望对其进行褒义或贬义的分类。当获得该文本的向量表示之后，就可以通过一个简单的分类器，如多层感知器，来判断该文本的类别
   ![[Pasted image 20230702212759.png]]

**词袋模型**    [[第四章#^5f7b03]]

除了用01来表示该词在文本中是否出现外，还可以用TF-IDF的值来更好的捕捉统计其分布式特征。[[TF-IDF]]
**词袋模型是词的独热表示在更长文本上的直接扩展**。
对于词袋模型的一种简单直接的分布式表示扩展是神经词缎懒型(eg of-words m0del。它对文本中所有单词的分布式表示向量求平均，从而得到该文本的分布式表示。**和一般的词袋模型一样，该方法也忽略了语法及同序的影响**。
另外，**也没有考虑不同的词对于文本整体语义的重要性差异**。
当然，可以使用TF-DF来针算词向量的加权平均，但是依然过于粗糙。

**注意力模型**:
对于注意力模型的一个重要的扩展是自注意力(self-aention）模型。自注意力模型为每一时刻的隐状态计算一个单独的查询向量，并计算其在文本中的注意力分布。这样一来，模型就能够学习文本内部词与词之间的依赖关系，从而捕获其内在结构。




