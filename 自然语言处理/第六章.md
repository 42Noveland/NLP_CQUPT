**n元语言模型**： 

**n元语法**

语言模型的基本任务是在给定词序列的情况下，对下一时刻可能出现的词wi的条件概率P(wi | w1w2w3...wi-1) 进行估计，一般的把w1w2w3...wi-1称为wi的历史

*基于频次统计的方法虽然简单易操作，但是随着句子长度的增加，需要估参数（条件概率）数量呈指数增长，为了合理估计这些参数所需的数据规模也随长。这一现象被称为**维度灾难(curse of dimensionality)**。*

对于上面的维度灾难等等问题，我们引入一个假设：下一个词的出现只依赖于离它最近的n-1个词，从形式上来看:
![[Pasted image 20230702213618.png]]
该假设被称为**马尔科夫假设**，满足这种假设的模型称为**N-gram模型（n元语法模型**）
特别的当n=1时，下一个词的出现独立于其历史，相应的一元语法通常记作**unigarm**.
n=2时，下一个词只依赖于其前一个词，对应的二元语法记作**bigram**。---此**二元语言模型**也被称为一阶**马尔科夫链**。
n=3时，也被称为二阶马尔科夫链。
由此可见，n的取值越大，参考的历史越完整。

**最大似然估计**：
如前所述，对于n元语法模型中条件概率的估计可以使用基于频次的方法，以二元语法模型（一阶马尔科夫链）为例。
![[Pasted image 20230702214321.png]]

这种方法被称为最大似然估计（maximum likelihood estimation,MLE）[[最大似然估计]]
![[Pasted image 20230702221358.png]]

![[Pasted image 20230702221409.png]]
**语言模型性能评价**：

用具体的外部任务，并根据任务上的指标变化来对语言模型进行的评价称为**外部任务评价** 。但是这种方法的计算代价比较高，因此，目前最为常用的仍然是基于**困惑度**的**内部评价方式**

为了进行**内部评价**，首先将数据划分为**不互相交**的两个集合，称为训练集和测试集。其中训练集用了训练模型，该模型计算出的概率p（测试集）反映了模型在测试集上的泛化能力。
![[Pasted image 20230702221433.png]]
**平滑**
基于最大似然估计的n元语法模型有一个潜在的缺点：**零概率问题**。
即在测试句子中存在**未登陆词**时，依据**mle**得到的句子概率将始终为0

为了避免这个问题，需要使用**平滑技术**来调整最大似然估计的结果。
1. 折扣法（**拉普拉斯平滑**）：
```   折扣法是一种简单的平滑技术，通过在估计的频次上添加一个小的常数（折扣因子），来调整最大似然估计的结果。这样可以避免估计的概率为零，同时减少数据稀疏性的影响。```
    具体地，对于一个观测到的事件的频次 c，折扣法的平滑估计可以表示为： P_smooth = (c + δ) / (N + δ * V)
    其中，P_smooth 是平滑后的概率估计值，c 是观测到的事件的频次，N 是总的观测数据数量，V 是事件的总数（词汇表大小），δ 是一个小的正常数，通常取1。
    折扣法通过在频次上添加 δ 进行平滑，使得即使某个事件没有在观测数据中出现，也能有一个非零的概率估计。
2. 插值法:
``  插值法是一种更复杂的平滑技术，它通过将多个 n-gram 模型的概率估计进行线性插值，来调整最大似然估计的结果。这样可以利用不同 n-gram 模型的信息来改善概率估计的准确性。``

具体地，对于一个 n-gram 模型，插值法的平滑估计可以表示为： P_smooth = λ₁ * P_n-gram + λ₂ *    P_n-1-gram + ... + λₖ * P_unigram   
其中，P_smooth 是平滑后的概率估计值，P_n-gram、P_n-1-gram、P_unigram 分别是不同 n-gram 模型的概率估计值，λ₁、λ₂、...、λₖ 是插值权重，满足 λ₁ + λ₂ + ... + λₖ = 1。  
插值法通过将不同 n-gram 模型的概率估计进行加权平均，来综合利用不同模型的信息，从而得到更准确的概率估计。
折扣法和插值法是常用的平滑技术，用于改善最大似然估计在维度灾难和数据稀疏性问题上的表现。具体选择哪种平滑技术取决于具体的应用场景和数据特征。

**前馈神经网络语言模型**：

前馈神经网络由输入层，隐含层，输出层构成

**循环神经网络语言模型**：[[循环神经网络]]

**预训练语言模型**：
在下游任务通常有三种使用预训练语言模型的**方式**：
1. 预训练语言模型直接作为生成器得到目标答案
2. 为下游任务提供词表示
3. 预训练语言模型本身作为下游任务模型的编码器

    **掩码预训练模型**：
    掩码语言模型（masked language model）不是通常意义上的语言模型，他的工作方式类似于**完形填空**的过程
![[Pasted image 20230702222134.png]]